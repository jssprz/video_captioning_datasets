{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate ActivityNet Dense Corpus, with programs and captions\n",
    "\n",
    "## Corpus file contents\n",
    "0. `train_data`: captions and idxs of training video segments in format `[corpus_opidxs, vidxs, intervals, fps]`, where:\n",
    "    - `corpus_opidxs` is a list of lists with the index of instructions (operations) in the vocabulary of operations\n",
    "    - `vidxs` is a list of indexes of video features in the features file\n",
    "    - `intervals` is a list of lists of tuples with the discretized intervals of each video\n",
    "    - `fps` is a list of the frame per seconds rate used to discretize the intervals\n",
    "    - `corpus_widxs` is a list of lists of lists with the index of words in the vocabulary of each caption of each video\n",
    "    - `corpus_pidxs` is a list of lists of lists with the index of POS tags in the POS tagging vocabulary of each caption of each video\n",
    "1. `val1_data`: same format as `train_data`.\n",
    "2. `val2_data`: same format as `train_data`.\n",
    "3. `programs_vocab`: in format `{'instruction': count}`.\n",
    "4. `idx2op`: is the vocabulary in format `{idx: 'instruction'}`.\n",
    "5. `caps_vocab`: in format `{'word': count}`.\n",
    "6. `idx2word`: is the vocabulary in format `{idx: 'word'}`.\n",
    "7. `word_embeddings`: are the vectors of each word. The *i*-th row is the word vector of the *i*-th word in the vocabulary.\n",
    "8. `idx2pos`: is the vocabulary of POS tagging in format `{idx: 'POSTAG'}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../../../data/ActivityNet/captions/train.json') as f:\n",
    "    datainfo_train = json.load(f)\n",
    "with open('../../../../data/ActivityNet/captions/val_1.json') as f:\n",
    "    datainfo_val_1 = json.load(f)\n",
    "with open('../../../../data/ActivityNet/captions/val_2.json') as f:\n",
    "    datainfo_val_2 = json.load(f)\n",
    "with open('../../../../data/ActivityNet/captions/train_ids.json') as f:\n",
    "    datainfo_train_ids = json.load(f)\n",
    "with open('../../../../data/ActivityNet/captions/val_ids.json') as f:\n",
    "    datainfo_val_ids = json.load(f)\n",
    "with open('../../../../data/ActivityNet/captions/test_ids.json') as f:\n",
    "    datainfo_test_ids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duration': 174.57,\n",
       " 'timestamps': [[0, 46.26],\n",
       "  [46.26, 71.57],\n",
       "  [72.45, 106.49],\n",
       "  [106.49, 110.85],\n",
       "  [110.85, 171.08]],\n",
       " 'sentences': ['People gather in a restaurant, then a cooker shows pasta and ingredients while talking.',\n",
       "  ' The cooker cuts and fries squid in a pot, then he adds salt and vinegar.',\n",
       "  ' Then, the cooker cut tomatoes and add to the squid, also adds garlic, pepper and green vegetables.',\n",
       "  ' After, the cooker adds water and covers the pot with aluminum paper.',\n",
       "  ' Next, the cooker add the pasta to the squid and mix, then he and serves in a dish while talking.']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datainfo_train['v_nlkmPF8TBdQ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing intervals distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max num of intervals: \"v_3l7quTy4c2s\" 27\n",
      "min num of intervals: \"v_nwznKOuZM7w\" 2\n",
      "avg num of intervals: 3.738735138375462\n",
      "stdev num of intervals: 1.8837039486692693\n",
      "total num of intervals: 37421\n"
     ]
    }
   ],
   "source": [
    "from statistics import stdev\n",
    "\n",
    "s = list(sorted(datainfo_train.items(), key=lambda item: len(item[1]['timestamps'])))\n",
    "\n",
    "M=s[-1]\n",
    "print('max num of intervals: \"{0}\" {1}'.format(M[0], len(M[1]['timestamps'])))\n",
    "\n",
    "m=s[0]\n",
    "print('min num of intervals: \"{0}\" {1}'.format(m[0], len(m[1]['timestamps'])))\n",
    "\n",
    "lens = [len(v[1]['timestamps']) for v in s]\n",
    "total=sum(lens)\n",
    "print(f'avg num of intervals: {total/len(s)}')\n",
    "print(f'stdev num of intervals: {stdev(lens)}')\n",
    "print(f'total num of intervals: {total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max interval: \"v_YtgiDWEY_1A\" [98.16, 505.93]\n"
     ]
    }
   ],
   "source": [
    "max_len, max_i, max_vidx = 0, None, None\n",
    "for k, data in datainfo_train.items():\n",
    "    for i in data['timestamps']:\n",
    "        if (i[1] - i[0]) > max_len:\n",
    "            max_vidx = k\n",
    "            max_len = i[1] - i[0]\n",
    "            max_i = i\n",
    "            \n",
    "print(f'max interval: \"{max_vidx}\" {max_i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping file for extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import imageio\n",
    "\n",
    "video_fps = {}\n",
    "\n",
    "def filter_vids(videos_folder, datainfo):\n",
    "    # determine the video-ids of videos that can be processed\n",
    "    correct_vids, error_vids = [], []\n",
    "    for vid, _ in datainfo.items():\n",
    "        path = os.path.join(videos_folder, vid+'.mp4')\n",
    "        try:\n",
    "            reader = imageio.get_reader(path)\n",
    "            video_fps[vid] = reader.get_meta_data()['fps']\n",
    "        except:\n",
    "            error_vids.append(vid)\n",
    "        else:\n",
    "            correct_vids.append(vid)\n",
    "        sys.stdout.write(f'\\rerrors {len(error_vids)}/{len(error_vids)+len(correct_vids)}')\n",
    "    return correct_vids, error_vids\n",
    "    \n",
    "# filter train set\n",
    "print('*****Processing train set*****')    \n",
    "train_vids, _ = filter_vids('../../../../data/ActivityNet/videos', datainfo_train)\n",
    "\n",
    "# filter train set\n",
    "print('\\n\\n*****Processing val_1 set*****')\n",
    "val_1_vids, _ = filter_vids('../../../../data/ActivityNet/videos', datainfo_val_1)\n",
    "\n",
    "# filter train set\n",
    "print('\\n\\n*****Processing val_2 set*****')\n",
    "val_2_vids, _ = filter_vids('../../../../data/ActivityNet/videos', datainfo_val_2)\n",
    "\n",
    "# create list of indices for using an only h5 file of features\n",
    "# train_vidxs = list(range(len(train_vids)))\n",
    "# val_1_vidxs = list(range(len(train_vids), len(train_vids) + len(val_1_vids)))\n",
    "# val_2_vidxs = list(range(len(train_vids) + len(val_1_vids), len(train_vids) + len(val_1_vids) + len(val_2_vids)))\n",
    "\n",
    "# create list of videos with the order to be used for extracting features\n",
    "# with open('../../../../data/ActivityNet/list_for_extraction.txt', 'w') as fo:\n",
    "#     for vid in train_vids+val_1_vids+val_2_vids:\n",
    "#         fo.write(\"%s\\n\" % vid)\n",
    "\n",
    "# create list of indices for using an h5 file of features for each split\n",
    "train_vidxs = list(range(len(train_vids)))\n",
    "val_1_vidxs = list(range(len(val_1_vids)))\n",
    "val_2_vidxs = list(range(len(val_2_vids)))\n",
    "\n",
    "# create list of videos with the order to be used for extracting features\n",
    "for split, vids in zip(['train', 'val_1', 'val_2'], [train_vids, val_1_vids, val_2_vids]):\n",
    "    with open(f'../../../../data/ActivityNet/{split}_list_for_extraction.txt', 'w') as fo:\n",
    "        for vid in vids:\n",
    "            fo.write(\"%s\\n\" % vid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update ground-truth references jeson files (for dense evaluation only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_datainfo_val_1 = {vidx: datainfo_val_1[vid] for vidx, vid in zip(val_1_vidxs, val_1_vids)}\n",
    "with open('../../results/ActivityNet-Dense_val_1_ref_densecap.json', 'w') as f:\n",
    "    json.dump(new_datainfo_val_1, f)\n",
    "    \n",
    "new_datainfo_val_2 = {vidx: datainfo_val_2[vid] for vidx, vid in zip(val_2_vidxs, val_2_vids)}\n",
    "with open('../../results/ActivityNet-Dense_val_2_ref_densecap.json', 'w') as f:\n",
    "    json.dump(new_datainfo_val_2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2op = {0:'<eos>', 1:'<unk>', 2:'skip', 3:'enqueue', 4:'generate'}\n",
    "op2idx = {'<eos>':0, '<unk>':1, 'skip':2, 'enqueue':3, 'generate':4}\n",
    "EOS, UNK = 0, 1\n",
    "\n",
    "def get_program(intervals, num_chunks, complete_skips=True):\n",
    "    program = []\n",
    "    p,q = 0,1\n",
    "    for s,e in intervals:\n",
    "        while True:\n",
    "            if p < s:\n",
    "                program.append(op2idx['skip'])\n",
    "                p+=1\n",
    "                q=p+1\n",
    "            if p >= s:\n",
    "                if (q+1) > e:\n",
    "                    program.append(op2idx['generate'])\n",
    "                    break\n",
    "                else:\n",
    "                    program.append(op2idx['enqueue'])\n",
    "                    q+=1\n",
    "    if complete_skips:\n",
    "        while p < num_chunks:\n",
    "            program.append(op2idx['skip'])\n",
    "            p+=1\n",
    "    return program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of chunks in a video: 1414.0\n",
      "Sum number of chunks of all video: 3646350.0\n"
     ]
    }
   ],
   "source": [
    "frames_per_chunk = 16 \n",
    "\n",
    "def get_intervals_and_programs(vids, datainfo):\n",
    "    intervals, programs, fps, max_num_chunks, sum_num_chunks = [], [], [], 0, 0\n",
    "    for vid in vids:\n",
    "        # get the fps rate of video vidx\n",
    "        vfps = video_fps[vid] \n",
    "        fps.append(vfps)\n",
    "\n",
    "        # determine the time of each chunk, used to discretisize the video intervals\n",
    "        chunk_duration = frames_per_chunk / vfps \n",
    "\n",
    "        # convert timestamps to chunk-indeces\n",
    "        vintervals = [(ts[0]//chunk_duration, ts[1]//chunk_duration) for ts in datainfo[vid]['timestamps']]\n",
    "        intervals.append(vintervals)\n",
    "        \n",
    "        # get the number of chunks to be processed\n",
    "        num_chunks = datainfo[vid]['duration']//chunk_duration\n",
    "        if num_chunks > max_num_chunks:\n",
    "            max_num_chunks = num_chunks\n",
    "        sum_num_chunks += num_chunks\n",
    "\n",
    "        # get program\n",
    "        programs.append(get_program(vintervals, num_chunks) + [EOS])\n",
    "    return intervals, programs, fps, max_num_chunks, sum_num_chunks\n",
    "\n",
    "train_intervals, train_programs, train_fps, m1, s1 = get_intervals_and_programs(train_vids, datainfo_train)\n",
    "val_1_intervals, val_1_programs, val_1_fps, m2, s2 = get_intervals_and_programs(val_1_vids, datainfo_val_1)\n",
    "val_2_intervals, val_2_programs, val_2_fps, m3, s3 = get_intervals_and_programs(val_2_vids, datainfo_val_2)\n",
    "\n",
    "print(f'Max number of chunks in a video: {max([m1,m2,m3])}')\n",
    "print(f'Sum number of chunks of all video: {sum([s1,s2,s3])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ground-truth references files of programs (for evaluating the progemmer model only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../results/ActivityNet-Dense_val_1_ref_programs.txt', 'w') as f:\n",
    "    for vidx, prog in zip(val_1_vidxs, val_1_programs):\n",
    "        f.write('{}\\t{}\\n'.format(vidx, ' '.join([idx2op[iop] for iop in prog])))\n",
    "        \n",
    "with open('../../results/ActivityNet-Dense_val_2_ref_programs.txt', 'w') as f:\n",
    "    for vidx, prog in zip(val_2_vidxs, val_2_programs):\n",
    "        f.write('{}\\t{}\\n'.format(vidx, ' '.join([idx2op[iop] for iop in prog])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing generated programs distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**train set statistics**\n",
      "max len: 2829\n",
      "min len: 5\n",
      "avg len: 428\n",
      "stdev len: 266.8239265678711\n",
      "number of <eos>: 9077\n",
      "number of <unk>: 0\n",
      "number of skip: 1840467\n",
      "number of enqueue: 2003669\n",
      "number of generate: 33844\n",
      "\n",
      "**val-1 set statistics**\n",
      "max len: 2435\n",
      "min len: 5\n",
      "avg len: 430\n",
      "stdev len: 271.908609288794\n",
      "number of <eos>: 4454\n",
      "number of <unk>: 0\n",
      "number of skip: 905896\n",
      "number of enqueue: 991059\n",
      "number of generate: 15826\n",
      "\n",
      "**val-2 set statistics**\n",
      "max len: 2317\n",
      "min len: 5\n",
      "avg len: 434\n",
      "stdev len: 281.9635100672509\n",
      "number of <eos>: 4424\n",
      "number of <unk>: 0\n",
      "number of skip: 899987\n",
      "number of enqueue: 1004064\n",
      "number of generate: 15413\n",
      "\n",
      "train-vocab: {'<eos>': 9077, '<unk>': 0, 'skip': 1840467, 'enqueue': 2003669, 'generate': 33844}\n"
     ]
    }
   ],
   "source": [
    "def print_statics(programs):\n",
    "    programs_s = list(sorted(programs, key=lambda x: len(x)))\n",
    "    print(f'max len: {len(programs_s[-1])}')\n",
    "    print(f'min len: {len(programs_s[0])}')\n",
    "    print(f'avg len: {sum([len(p) for p in programs_s])//len(programs_s)}')\n",
    "    print(f'stdev len: {stdev([len(p) for p in programs_s])}')\n",
    "\n",
    "    vocab = {}\n",
    "    for op, i in op2idx.items():\n",
    "        vocab[op] = sum([p.count(i) for p in programs])\n",
    "        print(f'number of {op}: {vocab[op]}')\n",
    "        \n",
    "    return vocab\n",
    "        \n",
    "print('\\n**train set statistics**')\n",
    "programs_vocab = print_statics(train_programs)\n",
    "\n",
    "print('\\n**val-1 set statistics**')\n",
    "print_statics(val_1_programs)\n",
    "\n",
    "print('\\n**val-2 set statistics**')\n",
    "print_statics(val_2_programs)\n",
    "\n",
    "print(f'\\ntrain-vocab: {programs_vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "max distance between skips is 2644 in the program of video \"v_YtgiDWEY_1A\", with 2829 instructions\n",
      "program of \"v_YtgiDWEY_1A\": eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegsssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssseeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeegssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss<\n"
     ]
    }
   ],
   "source": [
    "# determine the maximum distance between two skips instructions\n",
    "skip_max_dist, skip_max_vid, skip_max_i = 0, None, 0\n",
    "for i, (vid, p) in enumerate(zip(train_vids, train_programs)):\n",
    "    skips_pos = [i for i, o in enumerate(p) if o == op2idx['skip']]\n",
    "    if len(skips_pos):\n",
    "        prev = skips_pos[0]\n",
    "        for pos in skips_pos[1:]:\n",
    "            if pos - prev > skip_max_dist:\n",
    "                skip_max_dist = pos - prev\n",
    "                skip_max_vid = vid\n",
    "                skip_max_i = i\n",
    "    else:\n",
    "        print(f'video w/o skips: {vid}')\n",
    "\n",
    "print(f'\\nmax distance between skips is {skip_max_dist} in the program of video \"{skip_max_vid}\", with {len(train_programs[skip_max_i])} instructions')\n",
    "print(f'program of \"{skip_max_vid}\":', ''.join([idx2op[i][0] for i in train_programs[skip_max_i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing program generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s', 'e', 'g', 's', 'g', 's', 'e', 'g', 's', 'e', 'e', 'e', 'g', 's', 'g', 's', 'g', 's', 'g', 's', 's']\n",
      "['s', 's', 's', 'g', 'e', 'e', 'g', 's', 's', 's', 's']\n"
     ]
    }
   ],
   "source": [
    "intervals = [(1,3), (2,3), (3,5), (4,8), (5,6), (6,7), (7,8)]\n",
    "num_chunks = 9\n",
    "print([idx2op[i][0] for i in get_program(intervals, num_chunks)])\n",
    "\n",
    "intervals = [(3,4), (3,6)]\n",
    "num_chunks = 7\n",
    "print([idx2op[i][0] for i in get_program(intervals, num_chunks)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index of captions and ground-truth references files (for evaluating the captioning model only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cidxs = []\n",
    "current_idx = 0\n",
    "for vid in train_vids:\n",
    "    count = len(datainfo_train[vid]['sentences'])\n",
    "    train_cidxs.append([idx for idx in range(current_idx, current_idx+count)])\n",
    "    current_idx += count\n",
    "\n",
    "val_1_cidxs = []\n",
    "with open('../../results/ActivityNet-Dense_val_1_ref_captions.txt', 'w') as f:\n",
    "    for vid in val_1_vids:\n",
    "        caps = datainfo_val_1[vid]['sentences']\n",
    "        cidxs = list(range(current_idx, current_idx+len(caps)))\n",
    "        val_1_cidxs.append(cidxs)\n",
    "        current_idx += len(caps)\n",
    "        for cidx, cap in zip(cidxs, caps):\n",
    "            f.write('{}\\t{}\\n'.format(cidx, cap.strip().replace('\\n','').lower()))\n",
    "\n",
    "val_2_cidxs = []\n",
    "with open('../../results/ActivityNet-Dense_val_2_ref_captions.txt', 'w') as f:\n",
    "    for vid in val_2_vids:\n",
    "        caps = datainfo_val_1[vid]['sentences']\n",
    "        cidxs = list(range(current_idx, current_idx+len(caps)))\n",
    "        val_2_cidxs.append(cidxs)\n",
    "        current_idx += len(caps)\n",
    "        for cidx, cap in zip(cidxs, caps):\n",
    "            f.write('{}\\t{}\\n'.format(cidx, cap.strip().replace('\\n','').lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "wordvectors = {}\n",
    "# with open('./glove.42B.300d.txt') as f:\n",
    "with open('../../tools/glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        s = line.strip().split(' ')\n",
    "        if len(s) == 301:\n",
    "            wordvectors[s[0]] = np.array(s[1:], dtype=float)\n",
    "    print(len(wordvectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the vocabulary from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jeperez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. count of words per caption: 14.720659496513415\n",
      "Count of unique words:  10167\n",
      "missing word: hulte\n",
      "missing word: intertubes\n",
      "missing word: javeline\n",
      "missing word: sideward\n",
      "missing word: jump-stilts\n",
      "missing word: woodfire\n",
      "missing word: unscrews\n",
      "missing word: leathe\n",
      "missing word: gymasts\n",
      "missing word: voleyball\n",
      "missing word: liquir\n",
      "missing word: sepperates\n",
      "missing word: eliptical\n",
      "missing word: rubic\n",
      "missing word: canoesport\n",
      "missing word: intertube\n",
      "missing word: cheer-leading\n",
      "missing word: kufiyyas\n",
      "missing word: sanitizes\n",
      "missing word: wearinf\n",
      "missing word: aiter\n",
      "missing word: unhooks\n",
      "missing word: thorougly\n",
      "missing word: absorbant\n",
      "missing word: parasails\n",
      "missing word: plyas\n",
      "missing word: wreslers\n",
      "missing word: sizzors\n",
      "missing word: gargles\n",
      "missing word: zucky\n",
      "missing word: -end-\n",
      "missing word: unboxing\n",
      "missing word: rubix\n",
      "missing word: gabs\n",
      "missing word: themiddle\n",
      "missing word: garnishments\n",
      "missing word: whie\n",
      "missing word: trakc\n",
      "missing word: forearm-mounted\n",
      "missing word: re-equips\n",
      "missing word: wall/\n",
      "missing word: watchim\n",
      "missing word: valqueire\n",
      "missing word: vincanitv\n",
      "missing word: hols\n",
      "missing word: stream/body\n",
      "missing word: knitting-related\n",
      "missing word: wakeboards\n",
      "missing word: talll\n",
      "missing word: superfloor\n",
      "missing word: selfie\n",
      "missing word: ingedients\n",
      "missing word: wtching\n",
      "missing word: beraths\n",
      "missing word: brushteeth\n",
      "missing word: tanktop\n",
      "missing word: groip\n",
      "missing word: acrossing\n",
      "missing word: shrit\n",
      "missing word: kisd\n",
      "missing word: fooseball\n",
      "missing word: riving\n",
      "missing word: occassionaly\n",
      "missing word: poweriser\n",
      "missing word: prvni\n",
      "missing word: kroky\n",
      "missing word: cheerdancing\n",
      "missing word: talknig\n",
      "missing word: gril\n",
      "missing word: e-pen\n",
      "missing word: 33,91\n",
      "missing word: howdini\n",
      "missing word: peolpe\n",
      "missing word: nanchucks\n",
      "missing word: lee35\n",
      "missing word: robohandle\n",
      "missing word: jet-skis\n",
      "missing word: brussell\n",
      "missing word: unning\n",
      "missing word: skaet\n",
      "missing word: excerise\n",
      "missing word: selfies\n",
      "missing word: sharpents\n",
      "missing word: nailpolish\n",
      "missing word: founder-\n",
      "missing word: eyeliners\n",
      "missing word: blowdrying\n",
      "missing word: turtled\n",
      "missing word: vassen\n",
      "missing word: snwoboarding\n",
      "missing word: 'lying\n",
      "missing word: huging\n",
      "missing word: wretling\n",
      "missing word: anothe\n",
      "missing word: thumbling\n",
      "missing word: hormonica\n",
      "missing word: rappeling\n",
      "missing word: scubba\n",
      "missing word: weedeat\n",
      "missing word: triangled\n",
      "missing word: maltersers\n",
      "missing word: maltesers\n",
      "missing word: stucks\n",
      "missing word: candlies\n",
      "missing word: exfoliater\n",
      "missing word: costome\n",
      "missing word: froward\n",
      "missing word: spining\n",
      "missing word: themix\n",
      "missing word: reddetails\n",
      "missing word: tube/pump\n",
      "missing word: 7c/fr\n",
      "missing word: leifels\n",
      "missing word: carious\n",
      "missing word: bad-mitten\n",
      "missing word: thowing\n",
      "missing word: baloons\n",
      "missing word: arow\n",
      "missing word: alrge\n",
      "missing word: arros\n",
      "missing word: suffleboard\n",
      "missing word: skiis\n",
      "missing word: dobris\n",
      "missing word: claires\n",
      "missing word: t-zone\n",
      "missing word: calfs\n",
      "missing word: mutiple\n",
      "missing word: stepercize\n",
      "missing word: pomades\n",
      "missing word: highfive\n",
      "missing word: beerpong\n",
      "missing word: barier\n",
      "missing word: wozkach\n",
      "missing word: specator\n",
      "missing word: pand\n",
      "missing word: cameraperson\n",
      "missing word: alking\n",
      "missing word: j-rod\n",
      "missing word: rying\n",
      "missing word: catched\n",
      "missing word: bowtech\n",
      "missing word: technques\n",
      "missing word: peforms\n",
      "missing word: elliptico\n",
      "missing word: anoter\n",
      "missing word: giiving\n",
      "missing word: beatiful\n",
      "missing word: bicycler\n",
      "missing word: keylime\n",
      "missing word: high-fiving\n",
      "missing word: iswatching\n",
      "missing word: tandumisgreat\n",
      "missing word: lades\n",
      "missing word: windersurfers\n",
      "missing word: hulahoop\n",
      "missing word: mxgp\n",
      "missing word: burguer\n",
      "missing word: cutted\n",
      "missing word: do0dge\n",
      "missing word: jumprope\n",
      "missing word: briefliy\n",
      "missing word: petrillose\n",
      "missing word: hjctv\n",
      "missing word: sew-in\n",
      "missing word: criquet\n",
      "missing word: rnning\n",
      "missing word: grabing\n",
      "missing word: midle\n",
      "missing word: putsion\n",
      "missing word: sissor\n",
      "missing word: tshirt\n",
      "missing word: daces\n",
      "missing word: volting\n",
      "missing word: brenmar\n",
      "missing word: uniiqu3\n",
      "missing word: hoope\n",
      "missing word: srronded\n",
      "missing word: arond\n",
      "missing word: superfresco\n",
      "missing word: uncaps\n",
      "missing word: redigering\n",
      "missing word: lagerlof\n",
      "missing word: opponet\n",
      "missing word: tam-tams\n",
      "missing word: uncrosses\n",
      "missing word: picke\n",
      "missing word: haighway\n",
      "missing word: ddiver\n",
      "missing word: lassoes\n",
      "missing word: sceeen\n",
      "missing word: pick-me-up\n",
      "missing word: girlsmakingdrinks\n",
      "missing word: thers\n",
      "missing word: jump-rope\n",
      "missing word: hand-stands\n",
      "missing word: sea-slug\n",
      "missing word: worcesterchire\n",
      "missing word: 10x3\n",
      "missing word: oponent\n",
      "missing word: mountins\n",
      "missing word: lighes\n",
      "missing word: stran\n",
      "missing word: 1912-2012\n",
      "missing word: apears\n",
      "missing word: cubing\n",
      "missing word: coean\n",
      "missing word: happyface\n",
      "missing word: tic-\n",
      "missing word: fighing\n",
      "missing word: trye\n",
      "missing word: unifroms\n",
      "missing word: counte\n",
      "missing word: w2k15\n",
      "missing word: straitens\n",
      "missing word: caucasion\n",
      "missing word: gymnasic\n",
      "missing word: q-tips\n",
      "missing word: blowdryer\n",
      "missing word: blowdrys\n",
      "missing word: brycebetts13z\n",
      "missing word: staystrong\n",
      "missing word: summersault\n",
      "missing word: playplace\n",
      "missing word: de-tangle\n",
      "missing word: shavea\n",
      "missing word: jackolantern\n",
      "missing word: intermediately\n",
      "missing word: re-inactments\n",
      "missing word: windshielf\n",
      "missing word: btrtv\n",
      "missing word: tick-tack-toe\n",
      "missing word: wter\n",
      "missing word: truing\n",
      "missing word: releases/throws\n",
      "missing word: unchains\n",
      "missing word: girlwearnig\n",
      "missing word: fornt\n",
      "missing word: gregoirie\n",
      "missing word: cabonara\n",
      "missing word: ulampinoy\n",
      "missing word: failcorporation\n",
      "missing word: millionairemaintenance\n",
      "missing word: orbitrek\n",
      "missing word: hogtie\n",
      "missing word: membrillo\n",
      "missing word: bervelillo\n",
      "missing word: trillizos\n",
      "missing word: wwearing\n",
      "missing word: brownish-reddish\n",
      "missing word: beveling\n",
      "missing word: ornamens\n",
      "missing word: insisde\n",
      "missing word: shving\n",
      "missing word: ceam\n",
      "missing word: fence/weapon\n",
      "missing word: weaing\n",
      "missing word: whislet\n",
      "missing word: handwash\n",
      "missing word: smils\n",
      "missing word: soluting\n",
      "missing word: volyball\n",
      "missing word: oofed\n",
      "missing word: separetly\n",
      "missing word: meyerco\n",
      "missing word: machette\n",
      "missing word: dodgeballs\n",
      "missing word: handbagbuyer\n",
      "missing word: screan\n",
      "missing word: scouping\n",
      "missing word: dibloc\n",
      "missing word: charicature\n",
      "missing word: balded\n",
      "missing word: inext\n",
      "missing word: climbong\n",
      "missing word: atached\n",
      "missing word: accordians\n",
      "missing word: reinserts\n",
      "missing word: pliea\n",
      "missing word: dirtbikes\n",
      "missing word: sailboards\n",
      "missing word: sreet\n",
      "missing word: woth\n",
      "missing word: ashing\n",
      "missing word: startch\n",
      "missing word: back-flipping\n",
      "missing word: woma\n",
      "missing word: pictues\n",
      "missing word: girl/toddler\n",
      "missing word: 00:34:08\n",
      "missing word: wmoan\n",
      "missing word: decoative\n",
      "missing word: aound\n",
      "missing word: diferents\n",
      "missing word: stits\n",
      "missing word: ambulence\n",
      "missing word: prostetic\n",
      "missing word: oegano\n",
      "missing word: birds/oil\n",
      "missing word: unbends\n",
      "missing word: congsa\n",
      "missing word: un-boxed\n",
      "missing word: videoing\n",
      "missing word: swimmming\n",
      "missing word: bungees\n",
      "missing word: bungeeing\n",
      "missing word: shufleboard\n",
      "missing word: hero3+black\n",
      "missing word: 'mowing\n",
      "missing word: superwide\n",
      "missing word: mainge\n",
      "missing word: skareboard\n",
      "missing word: lay-ups\n",
      "missing word: fiveing\n",
      "missing word: lifevest\n",
      "missing word: stils\n",
      "missing word: trasher\n",
      "missing word: ront\n",
      "missing word: screenboard\n",
      "missing word: fying\n",
      "missing word: haidler\n",
      "missing word: successfull\n",
      "missing word: pokemon-pallet-town\n",
      "missing word: receiveng\n",
      "missing word: pumkkin\n",
      "missing word: keps\n",
      "missing word: diviing\n",
      "missing word: ield\n",
      "missing word: professionaldq\n",
      "missing word: motoroboat\n",
      "missing word: jack-o-lanterns\n",
      "missing word: rinong\n",
      "missing word: backetball\n",
      "missing word: lacross\n",
      "missing word: steffisburg\n",
      "missing word: thuglife\n",
      "missing word: lemon-aid\n",
      "missing word: wuzler\n",
      "missing word: tshirts\n",
      "missing word: srings\n",
      "missing word: acrs\n",
      "missing word: bmxing\n",
      "missing word: cheerlading\n",
      "missing word: tighrope\n",
      "missing word: bounches\n",
      "missing word: basoon\n",
      "missing word: monkeysee\n",
      "missing word: micropone\n",
      "missing word: mmen\n",
      "missing word: rodry-go\n",
      "missing word: bveeees\n",
      "missing word: ticas\n",
      "missing word: kichen\n",
      "missing word: surfboarding\n",
      "missing word: putnig\n",
      "missing word: sendriute\n",
      "missing word: dreassed\n",
      "missing word: lleans\n",
      "missing word: rubarb\n",
      "missing word: teraces\n",
      "missing word: slikhaar\n",
      "missing word: clodden\n",
      "missing word: rodea\n",
      "missing word: collar/harness\n",
      "missing word: ownders\n",
      "missing word: peeople\n",
      "missing word: spanxs\n",
      "missing word: waching\n",
      "missing word: abada-capoeira\n",
      "missing word: snowbrush\n",
      "missing word: dealnig\n",
      "missing word: icerink\n",
      "missing word: hulahooping\n",
      "missing word: swaddles\n",
      "missing word: rstaurant\n",
      "missing word: thefloor\n",
      "missing word: anoher\n",
      "missing word: skiping\n",
      "missing word: crowls\n",
      "missing word: bgeins\n",
      "missing word: capoiera\n",
      "missing word: toeside\n",
      "missing word: drainer\n",
      "missing word: akayak\n",
      "missing word: corect\n",
      "missing word: personell\n",
      "missing word: preperation\n",
      "missing word: performaing\n",
      "missing word: capoeria\n",
      "missing word: perusers\n",
      "missing word: tam-\n",
      "missing word: carying\n",
      "missing word: eners\n",
      "missing word: compilation/tribute\n",
      "missing word: hlping\n",
      "missing word: stading\n",
      "missing word: scoo\n",
      "missing word: wwhite\n",
      "missing word: hiyraulic\n",
      "missing word: recomendations\n",
      "missing word: competiion\n",
      "missing word: bouncehouse\n",
      "missing word: perso\n",
      "missing word: playig\n",
      "missing word: timelapse\n",
      "missing word: trows\n",
      "missing word: vickermann\n",
      "missing word: stoya\n",
      "missing word: japenese\n",
      "missing word: dodge-ball\n",
      "missing word: wlking\n",
      "missing word: pacticing\n",
      "missing word: storl\n",
      "missing word: wallpaperlike\n",
      "missing word: arms/legs\n",
      "missing word: refeee\n",
      "missing word: arpund\n",
      "missing word: swiftly-moving\n",
      "missing word: pullnig\n",
      "missing word: inner-tubes\n",
      "missing word: inner-tube\n",
      "missing word: grssy\n",
      "missing word: hiting\n",
      "missing word: happydogz\n",
      "missing word: pumpking\n",
      "missing word: 678-887-9479\n",
      "missing word: longjump\n",
      "missing word: parasailer\n",
      "missing word: gymaist\n",
      "missing word: wallwik\n",
      "missing word: sumerge\n",
      "missing word: sb4ts\n",
      "missing word: pongs\n",
      "missing word: i-pad\n",
      "missing word: qtip\n",
      "missing word: puntas\n",
      "missing word: wathing\n",
      "missing word: crochets\n",
      "missing word: re-wets\n",
      "missing word: coutner\n",
      "missing word: tattooo\n",
      "missing word: actiively\n",
      "missing word: tattooins\n",
      "missing word: vieing\n",
      "missing word: lttle\n",
      "missing word: throug\n",
      "missing word: aqueezing\n",
      "missing word: piramids\n",
      "missing word: masterlube\n",
      "missing word: 294-5530\n",
      "missing word: lamuscle\n",
      "missing word: isdoing\n",
      "missing word: liing\n",
      "missing word: stathom\n",
      "missing word: pushs\n",
      "missing word: hsown\n",
      "missing word: meled\n",
      "missing word: shot-put\n",
      "missing word: apperas\n",
      "missing word: preforming\n",
      "missing word: oilsand\n",
      "missing word: qtips\n",
      "missing word: bacyard\n",
      "missing word: rub-ix\n",
      "missing word: grop\n",
      "missing word: haging\n",
      "missing word: paits\n",
      "missing word: gymnist\n",
      "missing word: handwasher\n",
      "missing word: pivote\n",
      "missing word: fiair\n",
      "missing word: shissler\n",
      "missing word: handly\n",
      "missing word: scoeboard\n",
      "missing word: waltze\n",
      "missing word: nanogreenshawaii\n",
      "missing word: pajamma\n",
      "missing word: docto\n",
      "missing word: coffe\n",
      "missing word: qloja\n",
      "missing word: contraceptions\n",
      "missing word: choreogaphy\n",
      "missing word: thensee\n",
      "missing word: crispies\n",
      "missing word: peices\n",
      "missing word: vaccumed\n",
      "missing word: standingin\n",
      "missing word: scisoors\n",
      "missing word: congratlate\n",
      "missing word: reffing\n",
      "missing word: avoado\n",
      "missing word: skiin\n",
      "missing word: cherleader\n",
      "missing word: savegre\n",
      "missing word: saillboat\n",
      "missing word: hundred-point\n",
      "missing word: wearnig\n",
      "missing word: holdgina\n",
      "missing word: roofedwooden\n",
      "missing word: tan-tam\n",
      "missing word: cigarrette\n",
      "missing word: re-serve\n",
      "missing word: wildabeast\n",
      "missing word: shoing\n",
      "missing word: shownig\n",
      "missing word: one-seater\n",
      "missing word: gruop\n",
      "missing word: tomtatos\n",
      "missing word: backflipping\n",
      "missing word: claing\n",
      "missing word: jumpstilts\n",
      "missing word: repeel\n",
      "missing word: ruun\n",
      "missing word: 6'11\n",
      "missing word: albertinho\n",
      "missing word: gazio\n",
      "missing word: snowblowing\n",
      "missing word: stuntsamazing\n",
      "missing word: gyjmnast\n",
      "missing word: theksiny\n",
      "missing word: striaght\n",
      "missing word: braks\n",
      "missing word: leichtathletik\n",
      "missing word: gemacht\n",
      "missing word: diskus\n",
      "missing word: olympiasieger\n",
      "missing word: dickus\n",
      "missing word: gymast\n",
      "missing word: grabbs\n",
      "missing word: skurfing\n",
      "missing word: don'ts\n",
      "missing word: dodgbeall\n",
      "missing word: epbf\n",
      "missing word: paint-supply\n",
      "missing word: fehlstart\n",
      "missing word: adish\n",
      "missing word: immidiately\n",
      "missing word: megazip\n",
      "missing word: saxaphone\n",
      "missing word: nun-chucks\n",
      "missing word: jack-o-lantern\n",
      "missing word: bathrrom\n",
      "missing word: rollerbldes\n",
      "missing word: accesories\n",
      "missing word: cheerleader-related\n",
      "missing word: wife-beater\n",
      "missing word: papertowel\n",
      "missing word: icetrack\n",
      "missing word: round-about\n",
      "missing word: somervault\n",
      "missing word: team-flips\n",
      "missing word: grouts\n",
      "missing word: ditra\n",
      "missing word: weeds/marsh\n",
      "missing word: kics\n",
      "missing word: floorworks\n",
      "missing word: powerchalk\n",
      "missing word: ractrack\n",
      "missing word: kitesufing\n",
      "missing word: chiopped\n",
      "missing word: skip/hop\n",
      "missing word: woodn\n",
      "missing word: marchign\n",
      "missing word: sno-bond\n",
      "missing word: exercizing\n",
      "missing word: 'breathe\n",
      "missing word: keri-anne\n",
      "missing word: passerbys\n",
      "missing word: primps\n",
      "missing word: 73km\n",
      "missing word: 81km\n",
      "missing word: shurgs\n",
      "missing word: bulls-eye\n",
      "missing word: 'robin\n",
      "missing word: shis\n",
      "missing word: mulitple\n",
      "missing word: seppe\n",
      "missing word: kleva\n",
      "missing word: serated\n",
      "missing word: giffenig\n",
      "missing word: cirle\n",
      "missing word: samething\n",
      "missing word: scubadiving\n",
      "missing word: surronded\n",
      "missing word: hoolahoops\n",
      "missing word: hoolahoop\n",
      "missing word: tablein\n",
      "missing word: mmxiv\n",
      "missing word: hyperlite\n",
      "missing word: runnign\n",
      "missing word: talkign\n",
      "missing word: countsdown\n",
      "missing word: pretents\n",
      "missing word: estatic\n",
      "missing word: automaic\n",
      "missing word: tonue\n",
      "missing word: wearng\n",
      "missing word: swimmerd\n",
      "missing word: fitcast\n",
      "missing word: palloff\n",
      "missing word: jet-skiing\n",
      "missing word: jenki\n",
      "missing word: hurkies\n",
      "missing word: acuspike\n",
      "missing word: ofice\n",
      "missing word: tattoing\n",
      "missing word: tatto\n",
      "missing word: ovebe\n",
      "missing word: shos\n",
      "missing word: wriing\n",
      "missing word: pocks\n",
      "count of missing words:  610\n",
      "9557 9559 9559\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "caps_vocab, total_len, caps_count = {}, 0, 0\n",
    "for vid in train_vids:\n",
    "    caps = datainfo_train[vid]['sentences']\n",
    "    caps_count += len(caps)\n",
    "    for _, cap in enumerate(caps):\n",
    "        tokens = nltk.word_tokenize(cap.lower())\n",
    "        total_len += len(tokens)\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                caps_vocab[w] += 1\n",
    "            except:\n",
    "                caps_vocab[w] = 1\n",
    "\n",
    "print('Avg. count of words per caption:', total_len/caps_count)\n",
    "print('Count of unique words: ', len(caps_vocab))\n",
    "\n",
    "to_del = []\n",
    "for w in caps_vocab.keys():\n",
    "    if not w in wordvectors:\n",
    "        to_del.append(w)\n",
    "        print('missing word: {}'.format(w))\n",
    "\n",
    "print('count of missing words: ', len(to_del))\n",
    "        \n",
    "for w in to_del:\n",
    "    del caps_vocab[w]\n",
    "        \n",
    "idx2word = {idx: word for idx, word in enumerate(['<eos>', '<unk>'] + list(caps_vocab.keys()))}\n",
    "word2idx = {word: idx for idx, word in enumerate(['<eos>', '<unk>'] + list(caps_vocab.keys()))}\n",
    "EOS, UNK = 0, 1\n",
    "\n",
    "print(len(caps_vocab), len(idx2word), len(word2idx))\n",
    "\n",
    "word_embeddings = np.zeros((len(idx2word), 300))\n",
    "for idx, word in idx2word.items():\n",
    "    if idx == EOS:\n",
    "        word_embeddings[idx] = wordvectors['eos']\n",
    "    elif idx == UNK:\n",
    "        word_embeddings[idx] = wordvectors['unk']\n",
    "    else:\n",
    "        word_embeddings[idx] = wordvectors[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine POS-tagging vocabulary from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words per tag:\n",
      " DT:\t25\n",
      " JJ:\t2063\n",
      " NN:\t4672\n",
      " VBZ:\t1045\n",
      " VBN:\t642\n",
      " VBG:\t968\n",
      " IN:\t146\n",
      " CC:\t15\n",
      " PRP$:\t7\n",
      " .:\t3\n",
      " NNS:\t2056\n",
      " PRP:\t21\n",
      " VBP:\t693\n",
      " TO:\t1\n",
      " POS:\t3\n",
      " VB:\t973\n",
      " CD:\t113\n",
      " RB:\t470\n",
      " RP:\t28\n",
      " ,:\t1\n",
      " EX:\t6\n",
      " VBD:\t477\n",
      " WRB:\t7\n",
      " WDT:\t7\n",
      " JJS:\t18\n",
      " JJR:\t62\n",
      " WP:\t4\n",
      " RBR:\t20\n",
      " RBS:\t3\n",
      " '':\t2\n",
      " MD:\t15\n",
      " ``:\t2\n",
      " PDT:\t13\n",
      " WP$:\t1\n",
      " (:\t1\n",
      " ):\t1\n",
      " ::\t3\n",
      " NNP:\t11\n",
      " FW:\t18\n",
      " #:\t1\n",
      " SYM:\t1\n",
      " $:\t1\n",
      " UH:\t1\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "pos_vocab, pos_unique_words = {}, {}\n",
    "for vid in train_vids:\n",
    "    caps = datainfo_train[vid]['sentences']\n",
    "    for cap in caps:\n",
    "        for tag in nltk.pos_tag(nltk.word_tokenize(cap.lower())):\n",
    "            try:\n",
    "                pos_vocab[tag[1]] += 1\n",
    "                try: \n",
    "                    pos_unique_words[tag[1]][tag[0]] += 1\n",
    "                except:\n",
    "                    pos_unique_words[tag[1]][tag[0]] = 1\n",
    "            except:\n",
    "                pos_vocab[tag[1]] = 1\n",
    "                pos_unique_words[tag[1]] = {tag[0]: 1}\n",
    "\n",
    "print('Unique words per tag:')\n",
    "print('\\n'.join([f' {k}:\\t{len(words)}' for k, words in pos_unique_words.items()]))\n",
    "            \n",
    "idx2pos = {idx: word for idx, word in enumerate(['eos', 'unk'] + list(pos_vocab.keys()))}\n",
    "pos2idx = {word: idx for idx, word in enumerate(['eos', 'unk'] + list(pos_vocab.keys()))}\n",
    "EOS, UNK = 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Universal POS-tagging from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jeperez/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words per universal tag:\n",
      " DET:\t45\n",
      " ADJ:\t2134\n",
      " NOUN:\t6541\n",
      " VERB:\t3749\n",
      " ADP:\t146\n",
      " CONJ:\t15\n",
      " PRON:\t32\n",
      " .:\t15\n",
      " PRT:\t32\n",
      " NUM:\t113\n",
      " ADV:\t493\n",
      " X:\t20\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "upos_vocab, upos_unique_words = {}, {}\n",
    "for vid in train_vids:\n",
    "    caps = datainfo_train[vid]['sentences']\n",
    "    for cap in caps:\n",
    "        for tag in nltk.pos_tag(nltk.word_tokenize(cap.lower()), tagset='universal'):\n",
    "            try:\n",
    "                upos_vocab[tag[1]] += 1\n",
    "                try: \n",
    "                    upos_unique_words[tag[1]][tag[0]] += 1\n",
    "                except:\n",
    "                    upos_unique_words[tag[1]][tag[0]] = 1\n",
    "            except:\n",
    "                upos_vocab[tag[1]] = 1\n",
    "                upos_unique_words[tag[1]] = {tag[0]: 1}\n",
    "\n",
    "print('Unique words per universal tag:')\n",
    "print('\\n'.join([f' {k}:\\t{len(words)}' for k, words in upos_unique_words.items()]))\n",
    "            \n",
    "idx2upos = {idx: word for idx, word in enumerate(['eos', 'unk'] + list(upos_vocab.keys()))}\n",
    "upos2idx = {word: idx for idx, word in enumerate(['eos', 'unk'] + list(upos_vocab.keys()))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate corpus .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_corpus_widxs = [[[word2idx[w] if w in caps_vocab else UNK for w in nltk.word_tokenize(cap.lower())] + [EOS] for cap in datainfo_train[vid]['sentences']] for vid in train_vids]\n",
    "val_1_corpus_widxs = [[[word2idx[w] if w in caps_vocab else UNK for w in nltk.word_tokenize(cap.lower())] + [EOS] for cap in datainfo_val_1[vid]['sentences']] for vid in val_1_vids]\n",
    "val_2_corpus_widxs = [[[word2idx[w] if w in caps_vocab else UNK for w in nltk.word_tokenize(cap.lower())] + [EOS] for cap in datainfo_val_2[vid]['sentences']] for vid in val_2_vids]\n",
    "\n",
    "train_corpus_pidxs = [[[pos2idx[w[1]] if w[1] in pos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()))] + [EOS] for cap in datainfo_train[vid]['sentences']] for vid in train_vids]\n",
    "val_1_corpus_pidxs = [[[pos2idx[w[1]] if w[1] in pos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()))] + [EOS] for cap in datainfo_val_1[vid]['sentences']] for vid in val_1_vids]\n",
    "val_2_corpus_pidxs = [[[pos2idx[w[1]] if w[1] in pos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()))] + [EOS] for cap in datainfo_val_2[vid]['sentences']] for vid in val_2_vids]\n",
    "\n",
    "train_corpus_upidxs = [[[upos2idx[w[1]] if w[1] in upos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()), tagset=\"universal\")] + [EOS] for cap in datainfo_train[vid]['sentences']] for vid in train_vids]\n",
    "val_1_corpus_upidxs = [[[upos2idx[w[1]] if w[1] in upos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()), tagset=\"universal\")] + [EOS] for cap in datainfo_val_1[vid]['sentences']] for vid in val_1_vids]\n",
    "val_2_corpus_upidxs = [[[upos2idx[w[1]] if w[1] in upos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()), tagset=\"universal\")] + [EOS] for cap in datainfo_val_2[vid]['sentences']] for vid in val_2_vids]\n",
    "\n",
    "train_data = [train_vidxs, train_cidxs, train_intervals, train_fps, train_programs, train_corpus_widxs, train_corpus_pidxs, train_corpus_upidxs]\n",
    "val_1_data = [val_1_vidxs, val_1_cidxs, val_1_intervals, val_1_fps, val_1_programs, val_1_corpus_widxs, val_1_corpus_pidxs, val_1_corpus_upidxs]\n",
    "val_2_data = [val_2_vidxs, val_2_cidxs, val_2_intervals, val_2_fps, val_2_programs, val_2_corpus_widxs, val_2_corpus_pidxs, val_2_corpus_upidxs]\n",
    "\n",
    "with open('../../../../data/ActivityNet/activitynet_dense_corpus.pkl', 'wb') as outfile:\n",
    "    pickle.dump([train_data, val_1_data, val_2_data, programs_vocab, idx2op, caps_vocab, idx2word, word_embeddings, idx2pos, idx2upos], outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17955"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vidxs) + len(val_1_vidxs) + len(val_2_vidxs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
